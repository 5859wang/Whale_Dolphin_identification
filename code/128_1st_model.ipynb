{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43674f53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-18T13:19:14.651447Z",
     "start_time": "2022-02-18T13:19:13.370303Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./Humpback-Whale-Identification-1st/')\n",
    "\n",
    "from models import *\n",
    "from utils import *\n",
    "import os\n",
    "import gc\n",
    "import cv2\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "\n",
    "# For data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Pytorch Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda import amp\n",
    "\n",
    "# Utils\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "# Sklearn Imports\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# For Image Models\n",
    "import timm\n",
    "\n",
    "# Albumentations for augmentations\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# For colored terminal text\n",
    "from colorama import Fore, Back, Style\n",
    "b_ = Fore.BLUE\n",
    "sr_ = Style.RESET_ALL\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# For descriptive error messages\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb4c15e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-18T13:19:14.670068Z",
     "start_time": "2022-02-18T13:19:14.652357Z"
    }
   },
   "outputs": [],
   "source": [
    "CONFIG = {\"seed\": 2022,\n",
    "          \"epochs\": 20,\n",
    "          \"img_size\": 128,\n",
    "          \"model_name\": \"tf_efficientnet_b0\",\n",
    "          \"num_classes\": 15587,\n",
    "          \"train_batch_size\": 128,\n",
    "          \"valid_batch_size\": 128,\n",
    "          \"learning_rate\": 1e-4,\n",
    "          \"scheduler\": 'CosineAnnealingLR',\n",
    "          \"min_lr\": 1e-6,\n",
    "          \"T_max\": 500,\n",
    "          \"weight_decay\": 1e-6,\n",
    "          \"n_fold\": 5,\n",
    "          \"n_accumulate\": 1,\n",
    "          \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
    "          # ArcFace Hyperparameters\n",
    "          \"s\": 30.0, \n",
    "          \"m\": 0.50,\n",
    "          \"ls_eps\": 0.0,\n",
    "          \"easy_margin\": False\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a416b7c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-18T13:19:14.678538Z",
     "start_time": "2022-02-18T13:19:14.670871Z"
    }
   },
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    '''Sets the seed of the entire notebook so results are the same every time we run.\n",
    "    This is for REPRODUCIBILITY.'''\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "set_seed(CONFIG['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c36b886",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-18T13:19:15.022535Z",
     "start_time": "2022-02-18T13:19:14.679560Z"
    }
   },
   "outputs": [],
   "source": [
    "ROOT_DIR = '../data/'\n",
    "TRAIN_DIR = '../data/train_images-128-128/'\n",
    "TEST_DIR = '../data/test_images-128-128/'\n",
    "\n",
    "def get_train_file_path(id):\n",
    "    return f\"{TRAIN_DIR}/{id}\"\n",
    "\n",
    "df = pd.read_csv(f\"{ROOT_DIR}/train.csv\")\n",
    "df['file_path'] = df['image'].apply(get_train_file_path)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "df['individual_id_map'] = encoder.fit_transform(df['individual_id'])\n",
    "\n",
    "with open(\"le.pkl\", \"wb\") as fp:\n",
    "    joblib.dump(encoder, fp)\n",
    "# encoder.inverse_transform([1636])\n",
    "\n",
    "skf = StratifiedKFold(n_splits=CONFIG['n_fold'])\n",
    "for fold, ( _, val_) in enumerate(skf.split(X=df, y=df.individual_id_map)):\n",
    "      df.loc[val_ , \"kfold\"] = fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f01a9085",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-18T13:19:15.025863Z",
     "start_time": "2022-02-18T13:19:15.023241Z"
    }
   },
   "outputs": [],
   "source": [
    "class HappyWhaleDataset(Dataset):\n",
    "    def __init__(self, df, transforms=None):\n",
    "        self.df = df\n",
    "        self.file_names = df['file_path'].values\n",
    "        self.labels = df['individual_id_map'].values\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.file_names[index]\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        if self.transforms:\n",
    "            img = self.transforms(image=img)[\"image\"]\n",
    "            \n",
    "        return {\n",
    "            'image': img,\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1849d5cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-18T13:19:15.033598Z",
     "start_time": "2022-02-18T13:19:15.026617Z"
    }
   },
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    \"train\": A.Compose([\n",
    "        A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.Rotate(limit=30, p=0.5),\n",
    "        A.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0\n",
    "            ),\n",
    "        ToTensorV2()], p=1.),\n",
    "    \n",
    "    \"valid\": A.Compose([\n",
    "        A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n",
    "        A.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0\n",
    "            ),\n",
    "        ToTensorV2()], p=1.)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1d4e0a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-18T13:19:15.052547Z",
     "start_time": "2022-02-18T13:19:15.034967Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_loaders(df, fold):\n",
    "    df_train = df[df.kfold != fold].reset_index(drop=True)\n",
    "    df_valid = df[df.kfold == fold].reset_index(drop=True)\n",
    "    \n",
    "    train_dataset = HappyWhaleDataset(df_train, transforms=data_transforms[\"train\"])\n",
    "    valid_dataset = HappyWhaleDataset(df_valid, transforms=data_transforms[\"valid\"])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=CONFIG['train_batch_size'], \n",
    "                              num_workers=2, shuffle=True, pin_memory=True, drop_last=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=CONFIG['valid_batch_size'], \n",
    "                              num_workers=2, shuffle=False, pin_memory=True)\n",
    "    \n",
    "    return train_loader, valid_loader\n",
    "\n",
    "train_loader, valid_loader = prepare_loaders(df, fold=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eef8c9b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-18T13:19:17.684854Z",
     "start_time": "2022-02-18T13:19:15.053295Z"
    }
   },
   "outputs": [],
   "source": [
    "num_classes = 15587\n",
    "model = model_whale(num_classes=num_classes, inchannels=3, model_name='senet154').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e46e6ec4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-18T13:19:17.690031Z",
     "start_time": "2022-02-18T13:19:17.685652Z"
    }
   },
   "outputs": [],
   "source": [
    "def fetch_scheduler(optimizer):\n",
    "    if CONFIG['scheduler'] == 'CosineAnnealingLR':\n",
    "        scheduler = lr_scheduler.CosineAnnealingLR(optimizer,T_max=CONFIG['T_max'], \n",
    "                                                   eta_min=CONFIG['min_lr'])\n",
    "    elif CONFIG['scheduler'] == 'CosineAnnealingWarmRestarts':\n",
    "        scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer,T_0=CONFIG['T_0'], \n",
    "                                                             eta_min=CONFIG['min_lr'])\n",
    "    elif CONFIG['scheduler'] == None:\n",
    "        return None\n",
    "        \n",
    "    return scheduler\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=CONFIG['learning_rate'], \n",
    "                       weight_decay=CONFIG['weight_decay'])\n",
    "scheduler = fetch_scheduler(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50c9b020",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-18T13:19:17.699833Z",
     "start_time": "2022-02-18T13:19:17.690784Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):\n",
    "    model.train()\n",
    "    dataset_size = 0\n",
    "    running_loss = 0.0\n",
    "    sum = 0\n",
    "    train_loss_sum = 0\n",
    "    train_top1_sum = 0\n",
    "    train_map5_sum = 0\n",
    "    \n",
    "    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    for step, data in bar:\n",
    "        optimizer.zero_grad()\n",
    "        images = data['image'].to(device, dtype=torch.float)\n",
    "        labels = data['label'].to(device, dtype=torch.long)\n",
    "        batch_size = images.size(0)\n",
    "        \n",
    "        global_feat, local_feat, results = model(images)\n",
    "        model.getLoss(global_feat, local_feat, results, labels)\n",
    "        loss = model.loss\n",
    "    \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0, norm_type=2)\n",
    "        optimizer.step()\n",
    "        results = torch.cat([torch.sigmoid(results), torch.ones_like(results[:, :1]).float().cuda() * 0.5], 1)\n",
    "        top1_batch = accuracy(results, labels, topk=(1,))[0]\n",
    "        map5_batch = mapk(labels, results, k=5)\n",
    "        loss = loss.data.cpu().numpy()\n",
    "        sum += 1\n",
    "        train_loss_sum += loss\n",
    "        train_top1_sum += top1_batch\n",
    "        train_map5_sum += map5_batch\n",
    "        \n",
    "        bar.set_postfix(Epoch=epoch, Train_Loss=loss,\n",
    "                        LR=optimizer.param_groups[0]['lr'])\n",
    "    gc.collect()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4225fa4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-18T13:19:17.707430Z",
     "start_time": "2022-02-18T13:19:17.700500Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# @torch.inference_mode()\n",
    "def valid_one_epoch(model, dataloader, device, epoch):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        dataset_size = 0\n",
    "        running_loss = 0.0\n",
    "        valid_loss, index_valid= 0, 0\n",
    "        all_results = []\n",
    "        all_labels = []\n",
    "\n",
    "        bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "        for step, data in bar:        \n",
    "            images = data['image'].to(device, dtype=torch.float)\n",
    "            labels = data['label'].to(device, dtype=torch.long)\n",
    "            global_feat, local_feat, results = model(images)\n",
    "            model.getLoss(global_feat, local_feat, results, labels)\n",
    "\n",
    "            results = torch.sigmoid(results)\n",
    "\n",
    "            all_results.append(results)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "            b = len(labels)\n",
    "            valid_loss += model.loss.data.cpu().numpy() * b\n",
    "            index_valid += b\n",
    "        all_results = torch.cat(all_results, 0)\n",
    "        all_labels = torch.cat(all_labels, 0)\n",
    "\n",
    "        map5s, top1s, top5s = [], [], []\n",
    "        if 1:\n",
    "            ts = np.linspace(0.1, 0.9, 9)\n",
    "            for t in ts:\n",
    "                results_t = torch.cat([all_results, torch.ones_like(all_results[:, :1]).float().cuda() * t], 1)\n",
    "                top1_, top5_ = accuracy(results_t, all_labels)\n",
    "                map5_ = mapk(all_labels, results_t, k=5)\n",
    "                map5s.append(map5_)\n",
    "                top1s.append(top1_)\n",
    "                top5s.append(top5_)\n",
    "            map5 = max(map5s)\n",
    "            i_max = map5s.index(map5)\n",
    "            top1 = top1s[i_max]\n",
    "            top5 = top5s[i_max]\n",
    "            best_t = ts[i_max]\n",
    "\n",
    "        valid_loss /= index_valid\n",
    "\n",
    "    return valid_loss, top1, top5, map5, best_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf5a7a8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-18T13:19:17.714863Z",
     "start_time": "2022-02-18T13:19:17.708513Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_training(model, optimizer, scheduler, device, num_epochs):\n",
    "    # To automatically log gradients\n",
    "#     wandb.watch(model, log_freq=100)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(\"[INFO] Using GPU: {}\\n\".format(torch.cuda.get_device_name()))\n",
    "    \n",
    "    start = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_epoch_loss = np.inf\n",
    "    history = defaultdict(list)\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1): \n",
    "        gc.collect()\n",
    "        train_epoch_loss = train_one_epoch(model, optimizer, scheduler, \n",
    "                                           dataloader=train_loader, \n",
    "                                           device=CONFIG['device'], epoch=epoch)\n",
    "        \n",
    "        val_epoch_loss, top1, top5, map5, best_t = valid_one_epoch(model, valid_loader, device=CONFIG['device'], \n",
    "                                         epoch=epoch)\n",
    "    \n",
    "        history['Train Loss'].append(train_epoch_loss)\n",
    "        history['Valid Loss'].append(val_epoch_loss)\n",
    "        \n",
    "#         Log the metrics\n",
    "#         wandb.log({\"Train Loss\": train_epoch_loss})\n",
    "#         wandb.log({\"Valid Loss\": val_epoch_loss})\n",
    "        print(f\"{b_}Validation Loss Improved ({best_epoch_loss} ---> {val_epoch_loss}) map5：{map5}\")\n",
    "        if val_epoch_loss <= best_epoch_loss:\n",
    "            best_epoch_loss = val_epoch_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            PATH = f\"model_{epoch}.pth\"\n",
    "            torch.save(model.state_dict(), './weight/'+PATH)\n",
    "            torch.save({\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'epoch': epoch,\n",
    "                    'best_t':best_t,\n",
    "                }, f'./weight/optimizer_{epoch}.pth')\n",
    "            # Save a model file from the current directory\n",
    "            print(f\"Model Saved{sr_}\")\n",
    "            \n",
    "        print()\n",
    "    \n",
    "    end = time.time()\n",
    "    time_elapsed = end - start\n",
    "    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n",
    "    print(\"Best Loss: {:.4f}\".format(best_epoch_loss))\n",
    "    \n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f46fc5",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-02-18T13:19:13.374Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using GPU: NVIDIA GeForce RTX 3090\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [01:02<00:00,  1.60s/it, Epoch=1, LR=0.0001, Train_Loss=1.4263262]\n",
      "100%|██████████| 8/8 [00:02<00:00,  3.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mValidation Loss Improved (inf ---> 1.4835968589782715) map5：0.0021666666666666666\n",
      "Model Saved\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [01:03<00:00,  1.64s/it, Epoch=2, LR=0.0001, Train_Loss=1.4158506]\n",
      "100%|██████████| 8/8 [00:02<00:00,  3.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mValidation Loss Improved (1.4835968589782715 ---> 1.499460768699646) map5：0.0013333333333333333\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 39/39 [01:02<00:00,  1.59s/it, Epoch=3, LR=0.0001, Train_Loss=1.4493095]\n",
      "100%|██████████| 8/8 [00:02<00:00,  3.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mValidation Loss Improved (1.4835968589782715 ---> 1.4983874588012696) map5：0.005666666666666667\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [01:02<00:00,  1.61s/it, Epoch=4, LR=0.0001, Train_Loss=1.4516565]\n",
      "100%|██████████| 8/8 [00:02<00:00,  4.05it/s]"
     ]
    }
   ],
   "source": [
    "model, history = run_training(model, optimizer, scheduler,\n",
    "                              device=CONFIG['device'],\n",
    "                              num_epochs=CONFIG['epochs'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "new_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
