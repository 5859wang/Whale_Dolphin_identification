{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6009e502",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-19T12:22:06.310492Z",
     "start_time": "2022-02-19T12:22:04.560770Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../../../tez/')\n",
    "sys.path.append('../../../Humpback-Whale-Identification-1st/')\n",
    "from models import *\n",
    "from dataSet import *\n",
    "from utils import *\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "import albumentations\n",
    "import pandas as pd\n",
    "import tez\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from sklearn import metrics, model_selection, preprocessing\n",
    "from tez.callbacks import EarlyStopping\n",
    "from tez.datasets import ImageDataset\n",
    "from torch.nn import functional as F\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d4319c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-19T12:22:06.318985Z",
     "start_time": "2022-02-19T12:22:06.315415Z"
    }
   },
   "outputs": [],
   "source": [
    "INPUT_PATH = \"../../../../data/\"\n",
    "IMAGE_PATH = \"../../../../data/train_images-128-128/\"\n",
    "MODEL_PATH = \"./weight/tez\"\n",
    "MODEL_NAME = 'whale'\n",
    "TRAIN_BATCH_SIZE = 64\n",
    "VALID_BATCH_SIZE = 128\n",
    "EPOCHS = 20\n",
    "IMAGE_SIZE = 128\n",
    "num_classes = 15587\n",
    "\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    os.makedirs(MODEL_PATH)\n",
    "\n",
    "model_backbone = model_whale(num_classes=num_classes, inchannels=3, model_name='senet154').cuda()\n",
    "\n",
    "def get_train_file_path(id):\n",
    "    return f\"{IMAGE_PATH}/{id}\"\n",
    "\n",
    "df_data = pd.read_csv('../../../../data/train.csv')\n",
    "df_data['file_path'] = df_data['image'].apply(get_train_file_path)\n",
    "encoder = LabelEncoder()\n",
    "df_data['individual_id_map'] = encoder.fit_transform(df_data['individual_id'])\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "for fold, ( _, val_) in enumerate(skf.split(X=df_data, y=df_data.individual_id_map)):\n",
    "      df_data.loc[val_ , \"kfold\"] = fold\n",
    "\n",
    "df_data.to_csv('../../../../data/train_folds.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e1c6abe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-19T12:22:09.500333Z",
     "start_time": "2022-02-19T12:22:09.420463Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qi/anaconda3/envs/new_env/lib/python3.7/site-packages/albumentations/augmentations/transforms.py:691: FutureWarning: This class has been deprecated. Please use CoarseDropout\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "train_aug = albumentations.Compose(\n",
    "    [\n",
    "        albumentations.RandomResizedCrop(IMAGE_SIZE, IMAGE_SIZE),\n",
    "        albumentations.Transpose(p=0.5),\n",
    "        albumentations.HorizontalFlip(p=0.5),\n",
    "        albumentations.VerticalFlip(p=0.5),\n",
    "        albumentations.ShiftScaleRotate(p=0.5),\n",
    "        albumentations.HueSaturationValue(\n",
    "            hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5\n",
    "        ),\n",
    "        albumentations.RandomBrightnessContrast(\n",
    "            brightness_limit=(-0.1, 0.1), contrast_limit=(-0.1, 0.1), p=0.5\n",
    "        ),\n",
    "        albumentations.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225],\n",
    "            max_pixel_value=255.0,\n",
    "            p=1.0,\n",
    "        ),\n",
    "        albumentations.CoarseDropout(p=0.5),\n",
    "        albumentations.Cutout(p=0.5),\n",
    "    ],\n",
    "    p=1.0,\n",
    ")\n",
    "\n",
    "valid_aug = albumentations.Compose(\n",
    "    [\n",
    "        albumentations.CenterCrop(IMAGE_SIZE, IMAGE_SIZE, p=1.0),\n",
    "        albumentations.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225],\n",
    "            max_pixel_value=255.0,\n",
    "            p=1.0,\n",
    "        ),\n",
    "    ],\n",
    "    p=1.0,\n",
    ")\n",
    "\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument(\"--fold\", type=int, required=True)\n",
    "#     args = parser.parse_args()\n",
    "#     current_fold = int(args.fold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cba89ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-19T12:22:09.559096Z",
     "start_time": "2022-02-19T12:22:09.548767Z"
    }
   },
   "outputs": [],
   "source": [
    "class Whalemodel(tez.Model):    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.effnet = model_backbone\n",
    "        self.step_scheduler_after = \"epoch\"\n",
    "\n",
    "    def monitor_metrics(self, outputs, targets):\n",
    "        if targets is None:\n",
    "            return {}\n",
    "\n",
    "        all_results = torch.cat([outputs], 0)\n",
    "        all_labels = torch.cat([targets], 0)\n",
    "        map5s = []\n",
    "        if 1:\n",
    "            ts = np.linspace(0.1, 0.9, 9)\n",
    "            for t in ts:\n",
    "                results_t = torch.cat([all_results, torch.ones_like(all_results[:, :1]).float().cuda() * t], 1)\n",
    "                map5_ = mapk(all_labels, results_t, k=5)\n",
    "                map5s.append(map5_)\n",
    "            map5 = max(map5s)\n",
    "#             i_max = map5s.index(map5)\n",
    "            \n",
    "#         accuracy = metrics.accuracy_score(targets, outputs)\n",
    "        return {\"accuracy\": map5}\n",
    "\n",
    "    def fetch_optimizer(self):\n",
    "        opt = torch.optim.Adam(self.parameters(), lr=3e-4)\n",
    "        return opt\n",
    "\n",
    "    def fetch_scheduler(self):\n",
    "        sch = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            self.optimizer, T_0=10, T_mult=1, eta_min=1e-6, last_epoch=-1\n",
    "        )\n",
    "        return sch\n",
    "\n",
    "    def forward(self, image, targets=None):\n",
    "        batch_size, _, _, _ = image.shape\n",
    "\n",
    "        global_feat, local_feat, results = self.effnet(image)\n",
    "#         x = F.adaptive_avg_pool2d(x, 1).reshape(batch_size, -1)\n",
    "#         outputs = self.out(self.dropout(x))\n",
    "\n",
    "        if targets is not None:\n",
    "            \n",
    "            self.effnet.getLoss(global_feat, local_feat, results, targets)\n",
    "            results = torch.sigmoid(results)\n",
    "#             loss = nn.CrossEntropyLoss()(outputs, targets)\n",
    "#             all_results = torch.cat(all_results, 0)\n",
    "#             all_labels = torch.cat(all_labels, 0)\n",
    "            metrics = self.monitor_metrics(results, targets)\n",
    "            return results, self.effnet.loss, metrics\n",
    "        return results, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbc8bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    current_fold = 0\n",
    "    dfx = pd.read_csv(os.path.join(INPUT_PATH, \"train_folds.csv\"))\n",
    "    df_train = dfx[dfx.kfold != current_fold].reset_index(drop=True)\n",
    "    df_valid = dfx[dfx.kfold == current_fold].reset_index(drop=True)\n",
    "    train_image_paths = [os.path.join(IMAGE_PATH, x) for x in df_train.image.values]\n",
    "    valid_image_paths = [os.path.join(IMAGE_PATH, x) for x in df_valid.image.values]\n",
    "    train_targets = df_train.individual_id_map.values\n",
    "    valid_targets = df_valid.individual_id_map.values\n",
    "\n",
    "    train_dataset = ImageDataset(\n",
    "        image_paths=train_image_paths,\n",
    "        targets=train_targets,\n",
    "        augmentations=train_aug,\n",
    "    )\n",
    "\n",
    "    valid_dataset = ImageDataset(\n",
    "        image_paths=valid_image_paths,\n",
    "        targets=valid_targets,\n",
    "        augmentations=valid_aug,\n",
    "    )\n",
    "    model = Whalemodel()\n",
    "    es = EarlyStopping(\n",
    "        monitor=\"valid_loss\",\n",
    "        model_path=os.path.join(MODEL_PATH, MODEL_NAME + f\"_fold_{current_fold}.bin\"),\n",
    "        patience=3,\n",
    "        mode=\"min\",\n",
    "    )\n",
    "    model.fit(\n",
    "        train_dataset,\n",
    "        valid_dataset=valid_dataset,\n",
    "        train_bs=TRAIN_BATCH_SIZE,\n",
    "        valid_bs=VALID_BATCH_SIZE,\n",
    "        device=\"cuda\",\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=[es],\n",
    "        fp16=True,\n",
    "    )\n",
    "    model.save(os.path.join(MODEL_PATH, MODEL_NAME + f\"_fold_{current_fold}.bin\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793b8e9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "new_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
